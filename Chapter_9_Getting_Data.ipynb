{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXG69LWgWrjSvi/9lfDxsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Data-Science-from-Scratch_-First-Principles-with-Python-by-Joel-Graus/blob/main/Chapter_9_Getting_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stdin and stdout\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c6JWoiMgcH3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV4F3mqSa8Qi"
      },
      "outputs": [],
      "source": [
        "#here is a script that reads in lines of text and\n",
        "#spits back out the ones that match a regular expression:\n",
        "# egrep.py\n",
        "import sys, re\n",
        "\n",
        "# sys.argv is the list of command-line arguments\n",
        "# sys.argv[0] is the name of the program itself\n",
        "# sys.argv[1] will be the regex specified at the command line\n",
        "regex = sys.argv[1]\n",
        "\n",
        "# for every line passed into the script\n",
        "for line in sys.stdin:\n",
        "    # if it matches the regex, write it to stdout\n",
        "    if re.search(regex, line):\n",
        "        sys.stdout.write(line)\n",
        "\n",
        "#And here’s one that counts the lines it receives and then writes out the count:\n",
        "# line_count.py\n",
        "import sys\n",
        "\n",
        "# sys.stdin is the standard input stream\n",
        "count = 0\n",
        "for line in sys.stdin:\n",
        "    count += 1\n",
        "\n",
        "# print goes to sys.stdout\n",
        "print (count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Files\n",
        "**The Basics of Text Files**\n"
      ],
      "metadata": {
        "id": "1wGxRG7whRp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 'r' means read-only\n",
        "file_for_reading = open('reading_file.txt', 'r')\n",
        "\n",
        "# 'w' is write—will destroy the file if it already exists!\n",
        "file_for_writing = open('writing_file.txt', 'w')\n",
        "\n",
        "# 'a' is append—for adding to the end of the file\n",
        "file_for_appending = open('appending_file.txt', 'a')\n",
        "\n",
        "# don't forget to close your files when you're done\n",
        "file_for_writing.close()\n",
        "\n",
        "#Because it is easy to forget to close your files, you should always use them in a with\n",
        "#block, at the end of which they will be closed automatically:\n",
        "\n",
        "with open('reading_file.txt', 'r') as file:\n",
        "    data = function_that_gets_data_from(f)\n",
        "\n",
        "# at this point f has already been closed, so don't try to use it\n",
        "process(data)\n",
        "\n",
        "#If you need to read a whole text file, you can just iterate over the lines of the file using\n",
        "#for:\n",
        "starts_with_hash = 0\n",
        "\n",
        "with open('input.txt','r') as f:\n",
        "    for line in file:\n",
        "        if re.match(\"^#\",line): # use a regex to see if it starts with '#'\n",
        "            starts_with_hash += 1\n",
        "\n",
        "def get_domain(email_address):\n",
        "    \"\"\"split on '@' and return the last piece\"\"\"\n",
        "    return email_address.split('@')[-1]\n",
        "\n",
        "with open('email_addresses.txt', 'r') as f:\n",
        "    domain_counts = Counter(get_domain(line.strip())\n",
        "                             for line in f if '@' in line)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bxy7Bj9nhVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Delimited Files**"
      ],
      "metadata": {
        "id": "V0kxCP3RkJ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if we had a tab-delimited file of stock prices:\n",
        "\n",
        "6/20/2014 AAPL 90.91\n",
        "6/20/2014 MSFT 41.68\n",
        "6/20/2014 FB 64.5\n",
        "6/19/2014 AAPL 91.86\n",
        "6/19/2014 MSFT 41.51\n",
        "6/19/2014 FB 64.34\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('tab_delimited_stock_prices.txt', 'rb') as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        date = row[0]\n",
        "        symbol = row[1]\n",
        "        closing_price = float(row[2])\n",
        "        print (date, symbol, closing_price)\n",
        "\n",
        "\n",
        "#If your file has headers:\n",
        "date:symbol:closing_price\n",
        "6/20/2014:AAPL:90.91\n",
        "6/20/2014:MSFT:41.68\n",
        "6/20/2014:FB:64.5\n",
        "\n",
        "#you can either skip the header row (with an initial call to reader.next()) or get each row\n",
        "#as a dict (with the headers as keys) by using csv.DictReader:\n",
        "with open('colon_delimited_stock_prices.txt', 'rb') as f:\n",
        "    reader = csv.DictReader(f, delimiter=':')\n",
        "    for row in reader:\n",
        "        date = row[\"date\"]\n",
        "        symbol = row[\"symbol\"]\n",
        "        closing_price = float(row[\"closing_price\"])\n",
        "        process(date, symbol, closing_price)\n",
        "\n"
      ],
      "metadata": {
        "id": "DvzJctaFkP32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You can similarly write out delimited data using csv.writer:**"
      ],
      "metadata": {
        "id": "emHGCFKD5bHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can similarly write out delimited data using csv.writer:\n",
        "import csv\n",
        "import chardet\n",
        "today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
        "\n",
        "with open('comma_delimited_stock_prices.txt','wb') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    for stock, price in today_prices.items():\n",
        "        writer.writerow([stock.encode('utf-8'), price.encode('utf-8')])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCUxOsgZ5jKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "csv.writer will do the right thing if your fields themselves have commas in them. Your\n",
        "own hand-rolled writer probably won’t. For example, if you attempt:\n"
      ],
      "metadata": {
        "id": "eVw4ay689G_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = [[\"test1\", \"success\", \"Monday\"],\n",
        "[\"test2\", \"success, kind of\", \"Tuesday\"],\n",
        "[\"test3\", \"failure, kind of\", \"Wednesday\"],\n",
        "[\"test4\", \"failure, utter\", \"Thursday\"]]\n",
        "\n",
        "# don't do this!\n",
        "with open('bad_csv.txt', 'wb') as f:\n",
        "    for row in results:\n",
        "        f.write(\",\".join(map(str, row))) # might have too many commas in it!\n",
        "        f.write(\"\\n\") # row might have newlines as well!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eKzpiIWQ9Iqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "with open('good_csv.txt', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Y7I0hlYT-7qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping the Web**\n",
        "### **HTML and the Parsing Thereof**\n"
      ],
      "metadata": {
        "id": "R54GWFIW_JSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install html5lib\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmodNG2uEAnH",
        "outputId": "2c6cf79c-8c6e-4bbf-a750-9d4d01105153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "html = requests.get(\"https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\").text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "#print(soup)\n",
        "\n",
        "\n",
        "#We’ll typically work with Tag objects, which correspond to the tags representing the\n",
        "#structure of an HTML page.\n",
        "#For example, to find the first <p> tag (and its contents) you can use:\n",
        "first_paragraph = soup.find('p') # or just soup.p\n",
        "print(first_paragraph)\n",
        "\n",
        "#You can get the text contents of a Tag using its text property:\n",
        "first_paragraph_text = soup.p.text\n",
        "first_paragraph_words = soup.p.text.split()\n",
        "\n",
        "#And you can extract a tag’s attributes by treating it like a dict:\n",
        "first_paragraph_id = soup.p['id'] # raises KeyError if no 'id'\n",
        "first_paragraph_id2 = soup.p.get('id') # returns None if no 'id'\n",
        "\n",
        "#You can get multiple tags at once:\n",
        "all_paragraphs = soup.find_all('p') # or just soup('p')\n",
        "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
        "\n",
        "#Frequently you’ll want to find tags with a specific class:\n",
        "important_paragraphs = soup('p', {'class' : 'important'})\n",
        "important_paragraphs2 = soup('p', 'important')\n",
        "important_paragraphs3 = [p for p in soup('p')\n",
        "if 'important' in p.get('class', [])]\n",
        "\n",
        "#And you can combine these to implement more elaborate logic. For example, if you want\n",
        "#to find every <span> element that is contained inside a <div> element, you could do this:\n",
        "# warning, will return the same span multiple times\n",
        "# if it sits inside multiple divs\n",
        "# be more clever if that's the case\n",
        "spans_inside_divs = [span\n",
        "for div in soup('div') # for each <div> on the page\n",
        "for span in div('span')] # find each <span> inside it\n",
        "\n"
      ],
      "metadata": {
        "id": "UC1UEmwE_S3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: O’Reilly Books About Data**\n"
      ],
      "metadata": {
        "id": "ZqHWleyLIu1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you don't have to split the url like this unless it needs to fit in a book\n",
        "url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
        "\"data.do?sortby=publicationDate&page=1\"\n",
        "soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
        "\n",
        "#A good first step is to find all of the td thumbtext tag elements:\n",
        "tds = soup('td', 'thumbtext')\n",
        "print (len(tds))\n",
        "\n",
        "#Next we’d like to filter out the videos.\n",
        "def is_video(td):\n",
        "    pricelabels = td('span', 'pricelabel')\n",
        "    return (len(pricelabels) == 1 and\n",
        "               pricelabels[0].text.strip().startswith(\"Video\"))\n",
        "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
        "       the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
        "\n",
        "\n",
        "print (len([td for td in tds if not is_video(td)]))\n",
        "\n",
        "#Now we’re ready to start pulling data out of the td elements. It looks like the book title is\n",
        "#the text inside the <a> tag inside the <div class=\"thumbheader\">:\n",
        "title = td.find(\"div\", \"thumbheader\").a.text\n",
        "\n",
        "#The author(s) are in the text of the AuthorName <div>. They are prefaced by a By (which\n",
        "#we want to get rid of) and separated by commas (which we want to split out, after which\n",
        "#we’ll need to get rid of spaces):\n",
        "author_name = td.find('div', 'AuthorName').text\n",
        "authors = [x.strip() for x in re.sub(\"^By \", \"\", author_name).split(\",\")]\n",
        "\n",
        "#The ISBN seems to be contained in the link that’s in the thumbheader <div>:\n",
        "isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
        "# re.match captures the part of the regex in parentheses\n",
        "isbn = re.match(\"/product/(.*)\\.do\", isbn_link).group(1)\n",
        "\n",
        "#And the date is just the contents of the <span class=\"directorydate\">:\n",
        "date = td.find(\"span\", \"directorydate\").text.strip()\n",
        "\n",
        "#Let’s put this all together into a function:\n",
        "def book_info(td):\n",
        "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
        "    extract the book's details and return a dict\"\"\"\n",
        "    title = td.find(\"div\", \"thumbheader\").a.text\n",
        "    by_author = td.find('div', 'AuthorName').text\n",
        "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
        "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
        "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
        "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
        "\n",
        "    return {\n",
        "        \"title\" : title,\n",
        "        \"authors\" : authors,\n",
        "        \"isbn\" : isbn,\n",
        "        \"date\" : date }\n",
        "\n",
        "#And now we’re ready to scrape:\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from time import sleep\n",
        "base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
        "\"data.do?sortby=publicationDate&page=\"\n",
        "\n",
        "books = []\n",
        "NUM_PAGES = 31\n",
        "for page_num in range(1,NUM_PAGES+1):\n",
        "    print(\"souping page\", page_num, \",\", len(books), \" found so far\")\n",
        "    url = base_url + str(page_num)\n",
        "    soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
        "    for td in soup('td', 'thumbtext'):\n",
        "        if not is_video(td):\n",
        "            books.append(book_info(td))\n",
        "    sleep(30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhuzZoCpI4xs",
        "outputId": "dcd13eff-4335-415a-a0c5-7472a6c4364c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'td' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2f3b9291d3c2>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Now we’re ready to start pulling data out of the td elements. It looks like the book title is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#the text inside the <a> tag inside the <div class=\"thumbheader\">:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thumbheader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#The author(s) are in the text of the AuthorName <div>. They are prefaced by a By (which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnK92Q4tTbpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# you don't have to split the url like this unless it needs to fit in a book\n",
        "url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
        "\"data.do?sortby=publicationDate&page=1\"\n",
        "soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
        "\n",
        "#A good first step is to find all of the td thumbtext tag elements:\n",
        "tds = soup('td', 'thumbtext')\n",
        "print (len(tds))\n",
        "\n",
        "#Next we’d like to filter out the videos.\n",
        "def is_video(td):\n",
        "    pricelabels = td('span', 'pricelabel')\n",
        "    return (len(pricelabels) == 1 and\n",
        "               pricelabels[0].text.strip().startswith(\"Video\"))\n",
        "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
        "       the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
        "\n",
        "\n",
        "print (len([td for td in tds if not is_video(td)]))\n",
        "\n",
        "#Now we’re ready to start pulling data out of the td elements. It looks like the book title is\n",
        "#the text inside the <a> tag inside the <div class=\"thumbheader\">:\n",
        "for td in tds:\n",
        "    if not is_video(td):\n",
        "        title = td.find(\"div\", \"thumbheader\").a.text\n",
        "\n",
        "        #The author(s) are in the text of the AuthorName <div>. They are prefaced by a By (which\n",
        "        #we want to get rid of) and separated by commas (which we want to split out, after which\n",
        "        #we’ll need to get rid of spaces):\n",
        "        author_name = td.find('div', 'AuthorName').text\n",
        "        authors = [x.strip() for x in author_name.split('\\n') if x.strip()]"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdOHo0ikTg7w",
        "outputId": "8e74db62-e4b4-43f1-81fe-810a29596b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Using APIs**"
      ],
      "metadata": {
        "id": "GT-_RiU32dxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#JSON (and XML)\n",
        "import json\n",
        "\n",
        "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
        "                \"author\" : \"Joel Grus\",\n",
        "                \"publicationYear\" : 2014,\n",
        "                \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
        "\n",
        "# parse the JSON to create a Python dict\n",
        "deserialized = json.loads(serialized)\n",
        "if \"data science\" in deserialized[\"topics\"]:\n",
        "    print (deserialized)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G5vSj7DpTa7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b96dde-6204-49cc-b373-58aa7739868d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Data Science Book', 'author': 'Joel Grus', 'publicationYear': 2014, 'topics': ['data', 'science', 'data science']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dateutil\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spLWt7r_7Qmp",
        "outputId": "f6c04d0d-725e-46bc-fc80-e6d4e335c403"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using an Unauthenticated API**"
      ],
      "metadata": {
        "id": "vpRZrkPB55Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "from collections import Counter\n",
        "\n",
        "endpoint = \"https://api.github.com/users/kilo11/repos\"\n",
        "repos = json.loads(requests.get(endpoint).text)\n",
        "print(repos)\n",
        "\n",
        "\n",
        "#from which you’ll probably only ever need the dateutil.parser.parse function:\n",
        "from dateutil.parser import parse\n",
        "\n",
        "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
        "month_counts = Counter(date.month for date in dates)\n",
        "weekday_counts = Counter(date.weekday() for date in dates)\n",
        "print(dates)\n",
        "print(month_counts)\n",
        "print(weekday_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6xVOi9w6A8p",
        "outputId": "fb764151-8881-4122-ce3d-b24fe2e7cd54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 64323887, 'node_id': 'MDEwOlJlcG9zaXRvcnk2NDMyMzg4Nw==', 'name': 'effective-octo-rotary-phone', 'full_name': 'kilo11/effective-octo-rotary-phone', 'private': False, 'owner': {'login': 'kilo11', 'id': 20686427, 'node_id': 'MDQ6VXNlcjIwNjg2NDI3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20686427?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/kilo11', 'html_url': 'https://github.com/kilo11', 'followers_url': 'https://api.github.com/users/kilo11/followers', 'following_url': 'https://api.github.com/users/kilo11/following{/other_user}', 'gists_url': 'https://api.github.com/users/kilo11/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/kilo11/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/kilo11/subscriptions', 'organizations_url': 'https://api.github.com/users/kilo11/orgs', 'repos_url': 'https://api.github.com/users/kilo11/repos', 'events_url': 'https://api.github.com/users/kilo11/events{/privacy}', 'received_events_url': 'https://api.github.com/users/kilo11/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/kilo11/effective-octo-rotary-phone', 'description': None, 'fork': False, 'url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone', 'forks_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/forks', 'keys_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/teams', 'hooks_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/hooks', 'issue_events_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/issues/events{/number}', 'events_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/events', 'assignees_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/assignees{/user}', 'branches_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/branches{/branch}', 'tags_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/tags', 'blobs_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/languages', 'stargazers_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/stargazers', 'contributors_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/contributors', 'subscribers_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/subscribers', 'subscription_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/subscription', 'commits_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/contents/{+path}', 'compare_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/merges', 'archive_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/downloads', 'issues_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/issues{/number}', 'pulls_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/labels{/name}', 'releases_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/releases{/id}', 'deployments_url': 'https://api.github.com/repos/kilo11/effective-octo-rotary-phone/deployments', 'created_at': '2016-07-27T16:18:37Z', 'updated_at': '2016-07-27T16:18:37Z', 'pushed_at': '2016-07-27T16:18:38Z', 'git_url': 'git://github.com/kilo11/effective-octo-rotary-phone.git', 'ssh_url': 'git@github.com:kilo11/effective-octo-rotary-phone.git', 'clone_url': 'https://github.com/kilo11/effective-octo-rotary-phone.git', 'svn_url': 'https://github.com/kilo11/effective-octo-rotary-phone', 'homepage': None, 'size': 0, 'stargazers_count': 0, 'watchers_count': 0, 'language': None, 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}, {'id': 64327998, 'node_id': 'MDEwOlJlcG9zaXRvcnk2NDMyNzk5OA==', 'name': 'https-anywhereworks.com-nflwswvs21eehoajor', 'full_name': 'kilo11/https-anywhereworks.com-nflwswvs21eehoajor', 'private': False, 'owner': {'login': 'kilo11', 'id': 20686427, 'node_id': 'MDQ6VXNlcjIwNjg2NDI3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20686427?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/kilo11', 'html_url': 'https://github.com/kilo11', 'followers_url': 'https://api.github.com/users/kilo11/followers', 'following_url': 'https://api.github.com/users/kilo11/following{/other_user}', 'gists_url': 'https://api.github.com/users/kilo11/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/kilo11/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/kilo11/subscriptions', 'organizations_url': 'https://api.github.com/users/kilo11/orgs', 'repos_url': 'https://api.github.com/users/kilo11/repos', 'events_url': 'https://api.github.com/users/kilo11/events{/privacy}', 'received_events_url': 'https://api.github.com/users/kilo11/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/kilo11/https-anywhereworks.com-nflwswvs21eehoajor', 'description': None, 'fork': False, 'url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor', 'forks_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/forks', 'keys_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/teams', 'hooks_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/hooks', 'issue_events_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/issues/events{/number}', 'events_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/events', 'assignees_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/assignees{/user}', 'branches_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/branches{/branch}', 'tags_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/tags', 'blobs_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/languages', 'stargazers_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/stargazers', 'contributors_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/contributors', 'subscribers_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/subscribers', 'subscription_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/subscription', 'commits_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/contents/{+path}', 'compare_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/merges', 'archive_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/downloads', 'issues_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/issues{/number}', 'pulls_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/labels{/name}', 'releases_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/releases{/id}', 'deployments_url': 'https://api.github.com/repos/kilo11/https-anywhereworks.com-nflwswvs21eehoajor/deployments', 'created_at': '2016-07-27T17:22:04Z', 'updated_at': '2016-07-27T17:22:04Z', 'pushed_at': '2016-07-27T17:22:05Z', 'git_url': 'git://github.com/kilo11/https-anywhereworks.com-nflwswvs21eehoajor.git', 'ssh_url': 'git@github.com:kilo11/https-anywhereworks.com-nflwswvs21eehoajor.git', 'clone_url': 'https://github.com/kilo11/https-anywhereworks.com-nflwswvs21eehoajor.git', 'svn_url': 'https://github.com/kilo11/https-anywhereworks.com-nflwswvs21eehoajor', 'homepage': None, 'size': 0, 'stargazers_count': 0, 'watchers_count': 0, 'language': None, 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}]\n",
            "[datetime.datetime(2016, 7, 27, 16, 18, 37, tzinfo=tzlocal()), datetime.datetime(2016, 7, 27, 17, 22, 4, tzinfo=tzlocal())]\n",
            "Counter({7: 2})\n",
            "Counter({2: 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finding API**"
      ],
      "metadata": {
        "id": "uGwnNMmB9v4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need data from a specific site, look for a developers or API section of the site for\n",
        "details, and try searching the Web for “python __ api” to find a library. There is a Rotten\n",
        "Tomatoes API for Python. There are multiple Python wrappers for the Klout API, for the\n",
        "Yelp API, for the IMDB API, and so on.\n",
        "If you’re looking for lists of APIs that have Python wrappers, two directories are at Python\n",
        "API and Python for Beginners.\n",
        "If you want a directory of web APIs more broadly (without Python wrappers necessarily),\n",
        "a good resource is Programmable Web, which has a huge directory of categorized APIs.\n",
        "And if after all that you can’t find what you need, there’s always scraping, the last refuge\n",
        "of the data scientist."
      ],
      "metadata": {
        "id": "MV9QU_ok956v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Using the Twitter APIs\n"
      ],
      "metadata": {
        "id": "GLQlsqCc_XnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twython\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yle1ExQf_xPg",
        "outputId": "e3c5caec-48ba-48aa-bb24-0b4f32e40e67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from twython) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from twython) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython) (2024.2.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.2)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Credentials\n"
      ],
      "metadata": {
        "id": "8EN-cdG5_qlK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}